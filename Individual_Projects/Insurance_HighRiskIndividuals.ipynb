{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Insurance_HighRiskIndividuals.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qiv5ouD3dJiz"
      },
      "source": [
        "# **Lab 6**\n",
        "# **Author: Sai Tallapragada**\n",
        "# **Date: 2/28/2020**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vx4DxUmZcHyl"
      },
      "source": [
        "# **Overview**\n",
        "\n",
        "In this lab, we are trying to see if we can predict who to give Carvan Insurance to and who to avoid based upon various factors where we do not want to do business with high risk individuals. Ultimately, we are trying to see if we can predict who to offer the carvan insurance policy by trying to identify what are the mportant features of people who bought carvan insurance policy in the past. We will be using Logistic regression, Logistic regression with stepwise selection based on p-values, Random Forest based on Principal component analysis feature selection, and Random Forest based on feature importance feature selection to compare the results and trying to understnad what are the most important features that we should fit our model upon.\n",
        "\n",
        "I used sources of code from Excercise 6 and I have also used the following sources to help finish this lab:[Source 1](https://towardsdatascience.com/building-a-logistic-regression-in-python-step-by-step-becd4d56c9c8) and [Source 2](https://towardsdatascience.com/machine-learning-step-by-step-6fbde95c455a).\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4fBkoJUkRrV5"
      },
      "source": [
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# feature selection using LassoCV\n",
        "from sklearn.feature_selection import SelectFromModel\n",
        "from sklearn.linear_model import LassoCV\n",
        "\n",
        "# forward/backward feature selection based on p-value\n",
        "import statsmodels.api as sm\n",
        "\n",
        "# feature selection based on low variance\n",
        "from sklearn.feature_selection import VarianceThreshold\n",
        "\n",
        "import os  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FeSxW6w4jJud"
      },
      "source": [
        "import pandas as pd \n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.decomposition import PCA\n",
        "%matplotlib inline\n",
        "# forward/backward feature selection based on p-value\n",
        "import statsmodels.api as sm\n",
        "from sklearn.preprocessing import StandardScaler"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JKhwz4ozZlme"
      },
      "source": [
        "# **Data**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "13aeBdQsY0Ff"
      },
      "source": [
        "df=pd.read_csv(\"/content/drive/MyDrive/INET 4061/caravan-insurance-challenge.csv\") # Importing the data from a local file located in a local directory\n",
        "aa = df.drop(['ORIGIN'], axis=1)\n",
        "yy = aa['CARAVAN']\n",
        "XX = aa.drop(['CARAVAN'], axis=1)\n",
        "# We are creating Training and Testing datasets below.\n",
        "testing=df[df['ORIGIN']==\"test\"]\n",
        "training=df[df['ORIGIN']==\"train\"]\n",
        "testing= testing.drop(['ORIGIN'], axis=1)\n",
        "training= training.drop(['ORIGIN'], axis=1)\n",
        "X_train=training.drop(['CARAVAN'], axis=1)\n",
        "y_train = training['CARAVAN']\n",
        "X_test=testing.drop(['CARAVAN'], axis=1)\n",
        "y_test = testing['CARAVAN']\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9ryFwIV7xBr2"
      },
      "source": [
        "We are importing the data from a local file. We are also splitting the training and the testing dataset based upon the already given 59% training set to 41% testing set split from the ORGIN column."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e4t58tmjZrsz"
      },
      "source": [
        "# **Exploratory Data Analysis**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 423
        },
        "id": "-1mbpXhpZXY7",
        "outputId": "b132b8f3-5ca1-4b9c-ca70-376fefa929b4"
      },
      "source": [
        "\n",
        "#yy is the CARVAN column in the dataset.\n",
        "print(yy[0:5]) # Displaying the first 5 values in the CARVAN dataset\n",
        "yy_num=aa[aa[\"CARAVAN\"]==0] #  We are storing all the vvalues of CARAVAN which equal 0 in yy_num\n",
        "print(\"Orginal Dataset Shape:\")\n",
        "print(aa.shape) # aa number of rows indicate the total number of rows for the entire CARVAN column as well as the dataset.\n",
        "print(\"The number of rows indicate the number of 0 values for CARAVAN in the orginal dataset:\")\n",
        "print(yy_num.shape) # yy_num number of rows indicate number of 0s in CARVAN column\n",
        "XX.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0    0\n",
            "1    0\n",
            "2    0\n",
            "3    0\n",
            "4    0\n",
            "Name: CARAVAN, dtype: int64\n",
            "Orginal Dataset Shape:\n",
            "(9822, 86)\n",
            "The number of rows indicate the number of 0 values for CARAVAN in the orginal dataset:\n",
            "(9236, 86)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>MOSTYPE</th>\n",
              "      <th>MAANTHUI</th>\n",
              "      <th>MGEMOMV</th>\n",
              "      <th>MGEMLEEF</th>\n",
              "      <th>MOSHOOFD</th>\n",
              "      <th>MGODRK</th>\n",
              "      <th>MGODPR</th>\n",
              "      <th>MGODOV</th>\n",
              "      <th>MGODGE</th>\n",
              "      <th>MRELGE</th>\n",
              "      <th>MRELSA</th>\n",
              "      <th>MRELOV</th>\n",
              "      <th>MFALLEEN</th>\n",
              "      <th>MFGEKIND</th>\n",
              "      <th>MFWEKIND</th>\n",
              "      <th>MOPLHOOG</th>\n",
              "      <th>MOPLMIDD</th>\n",
              "      <th>MOPLLAAG</th>\n",
              "      <th>MBERHOOG</th>\n",
              "      <th>MBERZELF</th>\n",
              "      <th>MBERBOER</th>\n",
              "      <th>MBERMIDD</th>\n",
              "      <th>MBERARBG</th>\n",
              "      <th>MBERARBO</th>\n",
              "      <th>MSKA</th>\n",
              "      <th>MSKB1</th>\n",
              "      <th>MSKB2</th>\n",
              "      <th>MSKC</th>\n",
              "      <th>MSKD</th>\n",
              "      <th>MHHUUR</th>\n",
              "      <th>MHKOOP</th>\n",
              "      <th>MAUT1</th>\n",
              "      <th>MAUT2</th>\n",
              "      <th>MAUT0</th>\n",
              "      <th>MZFONDS</th>\n",
              "      <th>MZPART</th>\n",
              "      <th>MINKM30</th>\n",
              "      <th>MINK3045</th>\n",
              "      <th>MINK4575</th>\n",
              "      <th>MINK7512</th>\n",
              "      <th>...</th>\n",
              "      <th>PWALAND</th>\n",
              "      <th>PPERSAUT</th>\n",
              "      <th>PBESAUT</th>\n",
              "      <th>PMOTSCO</th>\n",
              "      <th>PVRAAUT</th>\n",
              "      <th>PAANHANG</th>\n",
              "      <th>PTRACTOR</th>\n",
              "      <th>PWERKT</th>\n",
              "      <th>PBROM</th>\n",
              "      <th>PLEVEN</th>\n",
              "      <th>PPERSONG</th>\n",
              "      <th>PGEZONG</th>\n",
              "      <th>PWAOREG</th>\n",
              "      <th>PBRAND</th>\n",
              "      <th>PZEILPL</th>\n",
              "      <th>PPLEZIER</th>\n",
              "      <th>PFIETS</th>\n",
              "      <th>PINBOED</th>\n",
              "      <th>PBYSTAND</th>\n",
              "      <th>AWAPART</th>\n",
              "      <th>AWABEDR</th>\n",
              "      <th>AWALAND</th>\n",
              "      <th>APERSAUT</th>\n",
              "      <th>ABESAUT</th>\n",
              "      <th>AMOTSCO</th>\n",
              "      <th>AVRAAUT</th>\n",
              "      <th>AAANHANG</th>\n",
              "      <th>ATRACTOR</th>\n",
              "      <th>AWERKT</th>\n",
              "      <th>ABROM</th>\n",
              "      <th>ALEVEN</th>\n",
              "      <th>APERSONG</th>\n",
              "      <th>AGEZONG</th>\n",
              "      <th>AWAOREG</th>\n",
              "      <th>ABRAND</th>\n",
              "      <th>AZEILPL</th>\n",
              "      <th>APLEZIER</th>\n",
              "      <th>AFIETS</th>\n",
              "      <th>AINBOED</th>\n",
              "      <th>ABYSTAND</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>33</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>8</td>\n",
              "      <td>0</td>\n",
              "      <td>5</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>7</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>6</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>7</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>5</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>6</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>8</td>\n",
              "      <td>8</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>8</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>4</td>\n",
              "      <td>5</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>6</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>5</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>37</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>8</td>\n",
              "      <td>1</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>4</td>\n",
              "      <td>6</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>4</td>\n",
              "      <td>5</td>\n",
              "      <td>0</td>\n",
              "      <td>5</td>\n",
              "      <td>4</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>5</td>\n",
              "      <td>0</td>\n",
              "      <td>4</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>5</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>7</td>\n",
              "      <td>7</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>6</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>5</td>\n",
              "      <td>2</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>37</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>8</td>\n",
              "      <td>0</td>\n",
              "      <td>4</td>\n",
              "      <td>2</td>\n",
              "      <td>4</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>5</td>\n",
              "      <td>4</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>7</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>5</td>\n",
              "      <td>0</td>\n",
              "      <td>4</td>\n",
              "      <td>0</td>\n",
              "      <td>7</td>\n",
              "      <td>2</td>\n",
              "      <td>7</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>9</td>\n",
              "      <td>0</td>\n",
              "      <td>4</td>\n",
              "      <td>5</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>6</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>9</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>4</td>\n",
              "      <td>5</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>4</td>\n",
              "      <td>3</td>\n",
              "      <td>4</td>\n",
              "      <td>2</td>\n",
              "      <td>4</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>4</td>\n",
              "      <td>0</td>\n",
              "      <td>5</td>\n",
              "      <td>4</td>\n",
              "      <td>9</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>7</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>5</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>6</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>40</td>\n",
              "      <td>1</td>\n",
              "      <td>4</td>\n",
              "      <td>2</td>\n",
              "      <td>10</td>\n",
              "      <td>1</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>4</td>\n",
              "      <td>7</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>5</td>\n",
              "      <td>4</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>5</td>\n",
              "      <td>4</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>9</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>4</td>\n",
              "      <td>5</td>\n",
              "      <td>6</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>5</td>\n",
              "      <td>4</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>9</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>6</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows Ã— 85 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "   MOSTYPE  MAANTHUI  MGEMOMV  MGEMLEEF  ...  APLEZIER  AFIETS  AINBOED  ABYSTAND\n",
              "0       33         1        3         2  ...         0       0        0         0\n",
              "1       37         1        2         2  ...         0       0        0         0\n",
              "2       37         1        2         2  ...         0       0        0         0\n",
              "3        9         1        3         3  ...         0       0        0         0\n",
              "4       40         1        4         2  ...         0       0        0         0\n",
              "\n",
              "[5 rows x 85 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SyQWDN07ihRm"
      },
      "source": [
        "As we can see from above, the first 5 values from the CARAVAN column seem to be 0s. Further more, according to the shap of the variable aa and shape of the variable yy_num, there seems to be 9236 number of 0s in the CARAVAN column with only 586 number of 1s in the CARAVAN column. This is a very unbalnces distribution of data we are dealing with.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "adHoz2pDFEs4",
        "outputId": "18b955af-132c-4914-bf7b-6ab15b699df4"
      },
      "source": [
        "df.isnull().values.any() # Checking for any null/missing values in our dataset"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "False"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 59
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-GV0vn6aFV90"
      },
      "source": [
        "Since the following snippet of code above returned False, we can understand that there are no missing nor null values in our imported dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 346
        },
        "id": "JBxeZ0P5Zzos",
        "outputId": "a636792f-0d8d-4368-cb2a-05af4f3548fe"
      },
      "source": [
        "df.describe() # since they are as the same scale, no need for standardizaiton before feature selection."
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>MOSTYPE</th>\n",
              "      <th>MAANTHUI</th>\n",
              "      <th>MGEMOMV</th>\n",
              "      <th>MGEMLEEF</th>\n",
              "      <th>MOSHOOFD</th>\n",
              "      <th>MGODRK</th>\n",
              "      <th>MGODPR</th>\n",
              "      <th>MGODOV</th>\n",
              "      <th>MGODGE</th>\n",
              "      <th>MRELGE</th>\n",
              "      <th>MRELSA</th>\n",
              "      <th>MRELOV</th>\n",
              "      <th>MFALLEEN</th>\n",
              "      <th>MFGEKIND</th>\n",
              "      <th>MFWEKIND</th>\n",
              "      <th>MOPLHOOG</th>\n",
              "      <th>MOPLMIDD</th>\n",
              "      <th>MOPLLAAG</th>\n",
              "      <th>MBERHOOG</th>\n",
              "      <th>MBERZELF</th>\n",
              "      <th>MBERBOER</th>\n",
              "      <th>MBERMIDD</th>\n",
              "      <th>MBERARBG</th>\n",
              "      <th>MBERARBO</th>\n",
              "      <th>MSKA</th>\n",
              "      <th>MSKB1</th>\n",
              "      <th>MSKB2</th>\n",
              "      <th>MSKC</th>\n",
              "      <th>MSKD</th>\n",
              "      <th>MHHUUR</th>\n",
              "      <th>MHKOOP</th>\n",
              "      <th>MAUT1</th>\n",
              "      <th>MAUT2</th>\n",
              "      <th>MAUT0</th>\n",
              "      <th>MZFONDS</th>\n",
              "      <th>MZPART</th>\n",
              "      <th>MINKM30</th>\n",
              "      <th>MINK3045</th>\n",
              "      <th>MINK4575</th>\n",
              "      <th>MINK7512</th>\n",
              "      <th>...</th>\n",
              "      <th>PPERSAUT</th>\n",
              "      <th>PBESAUT</th>\n",
              "      <th>PMOTSCO</th>\n",
              "      <th>PVRAAUT</th>\n",
              "      <th>PAANHANG</th>\n",
              "      <th>PTRACTOR</th>\n",
              "      <th>PWERKT</th>\n",
              "      <th>PBROM</th>\n",
              "      <th>PLEVEN</th>\n",
              "      <th>PPERSONG</th>\n",
              "      <th>PGEZONG</th>\n",
              "      <th>PWAOREG</th>\n",
              "      <th>PBRAND</th>\n",
              "      <th>PZEILPL</th>\n",
              "      <th>PPLEZIER</th>\n",
              "      <th>PFIETS</th>\n",
              "      <th>PINBOED</th>\n",
              "      <th>PBYSTAND</th>\n",
              "      <th>AWAPART</th>\n",
              "      <th>AWABEDR</th>\n",
              "      <th>AWALAND</th>\n",
              "      <th>APERSAUT</th>\n",
              "      <th>ABESAUT</th>\n",
              "      <th>AMOTSCO</th>\n",
              "      <th>AVRAAUT</th>\n",
              "      <th>AAANHANG</th>\n",
              "      <th>ATRACTOR</th>\n",
              "      <th>AWERKT</th>\n",
              "      <th>ABROM</th>\n",
              "      <th>ALEVEN</th>\n",
              "      <th>APERSONG</th>\n",
              "      <th>AGEZONG</th>\n",
              "      <th>AWAOREG</th>\n",
              "      <th>ABRAND</th>\n",
              "      <th>AZEILPL</th>\n",
              "      <th>APLEZIER</th>\n",
              "      <th>AFIETS</th>\n",
              "      <th>AINBOED</th>\n",
              "      <th>ABYSTAND</th>\n",
              "      <th>CARAVAN</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>count</th>\n",
              "      <td>9822.000000</td>\n",
              "      <td>9822.000000</td>\n",
              "      <td>9822.000000</td>\n",
              "      <td>9822.000000</td>\n",
              "      <td>9822.000000</td>\n",
              "      <td>9822.000000</td>\n",
              "      <td>9822.000000</td>\n",
              "      <td>9822.000000</td>\n",
              "      <td>9822.000000</td>\n",
              "      <td>9822.000000</td>\n",
              "      <td>9822.000000</td>\n",
              "      <td>9822.000000</td>\n",
              "      <td>9822.000000</td>\n",
              "      <td>9822.000000</td>\n",
              "      <td>9822.000000</td>\n",
              "      <td>9822.000000</td>\n",
              "      <td>9822.000000</td>\n",
              "      <td>9822.000000</td>\n",
              "      <td>9822.000000</td>\n",
              "      <td>9822.000000</td>\n",
              "      <td>9822.000000</td>\n",
              "      <td>9822.000000</td>\n",
              "      <td>9822.000000</td>\n",
              "      <td>9822.000000</td>\n",
              "      <td>9822.000000</td>\n",
              "      <td>9822.000000</td>\n",
              "      <td>9822.000000</td>\n",
              "      <td>9822.000000</td>\n",
              "      <td>9822.000000</td>\n",
              "      <td>9822.000000</td>\n",
              "      <td>9822.000000</td>\n",
              "      <td>9822.000000</td>\n",
              "      <td>9822.000000</td>\n",
              "      <td>9822.000000</td>\n",
              "      <td>9822.000000</td>\n",
              "      <td>9822.000000</td>\n",
              "      <td>9822.000000</td>\n",
              "      <td>9822.000000</td>\n",
              "      <td>9822.000000</td>\n",
              "      <td>9822.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>9822.000000</td>\n",
              "      <td>9822.000000</td>\n",
              "      <td>9822.000000</td>\n",
              "      <td>9822.000000</td>\n",
              "      <td>9822.000000</td>\n",
              "      <td>9822.000000</td>\n",
              "      <td>9822.000000</td>\n",
              "      <td>9822.000000</td>\n",
              "      <td>9822.000000</td>\n",
              "      <td>9822.000000</td>\n",
              "      <td>9822.000000</td>\n",
              "      <td>9822.000000</td>\n",
              "      <td>9822.000000</td>\n",
              "      <td>9822.000000</td>\n",
              "      <td>9822.000000</td>\n",
              "      <td>9822.000000</td>\n",
              "      <td>9822.000000</td>\n",
              "      <td>9822.000000</td>\n",
              "      <td>9822.000000</td>\n",
              "      <td>9822.000000</td>\n",
              "      <td>9822.000000</td>\n",
              "      <td>9822.000000</td>\n",
              "      <td>9822.000000</td>\n",
              "      <td>9822.000000</td>\n",
              "      <td>9822.000000</td>\n",
              "      <td>9822.000000</td>\n",
              "      <td>9822.000000</td>\n",
              "      <td>9822.000000</td>\n",
              "      <td>9822.000000</td>\n",
              "      <td>9822.000000</td>\n",
              "      <td>9822.000000</td>\n",
              "      <td>9822.000000</td>\n",
              "      <td>9822.000000</td>\n",
              "      <td>9822.000000</td>\n",
              "      <td>9822.000000</td>\n",
              "      <td>9822.000000</td>\n",
              "      <td>9822.00000</td>\n",
              "      <td>9822.000000</td>\n",
              "      <td>9822.000000</td>\n",
              "      <td>9822.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mean</th>\n",
              "      <td>24.253207</td>\n",
              "      <td>1.108735</td>\n",
              "      <td>2.677561</td>\n",
              "      <td>2.996437</td>\n",
              "      <td>5.779067</td>\n",
              "      <td>0.700672</td>\n",
              "      <td>4.637650</td>\n",
              "      <td>1.050092</td>\n",
              "      <td>3.262981</td>\n",
              "      <td>6.188964</td>\n",
              "      <td>0.873142</td>\n",
              "      <td>2.286602</td>\n",
              "      <td>1.887294</td>\n",
              "      <td>3.237324</td>\n",
              "      <td>4.302891</td>\n",
              "      <td>1.484525</td>\n",
              "      <td>3.307269</td>\n",
              "      <td>4.592038</td>\n",
              "      <td>1.898799</td>\n",
              "      <td>0.403278</td>\n",
              "      <td>0.545714</td>\n",
              "      <td>2.877113</td>\n",
              "      <td>2.226532</td>\n",
              "      <td>2.291183</td>\n",
              "      <td>1.650682</td>\n",
              "      <td>1.595093</td>\n",
              "      <td>2.204744</td>\n",
              "      <td>3.742211</td>\n",
              "      <td>1.068214</td>\n",
              "      <td>4.187742</td>\n",
              "      <td>4.819487</td>\n",
              "      <td>6.022501</td>\n",
              "      <td>1.335980</td>\n",
              "      <td>1.956730</td>\n",
              "      <td>6.254327</td>\n",
              "      <td>2.750662</td>\n",
              "      <td>2.577072</td>\n",
              "      <td>3.505498</td>\n",
              "      <td>2.739462</td>\n",
              "      <td>0.808491</td>\n",
              "      <td>...</td>\n",
              "      <td>2.956424</td>\n",
              "      <td>0.054877</td>\n",
              "      <td>0.170841</td>\n",
              "      <td>0.008858</td>\n",
              "      <td>0.019344</td>\n",
              "      <td>0.093565</td>\n",
              "      <td>0.011505</td>\n",
              "      <td>0.215027</td>\n",
              "      <td>0.202301</td>\n",
              "      <td>0.011505</td>\n",
              "      <td>0.018733</td>\n",
              "      <td>0.023315</td>\n",
              "      <td>1.849420</td>\n",
              "      <td>0.001629</td>\n",
              "      <td>0.015272</td>\n",
              "      <td>0.025351</td>\n",
              "      <td>0.016697</td>\n",
              "      <td>0.045408</td>\n",
              "      <td>0.400020</td>\n",
              "      <td>0.014050</td>\n",
              "      <td>0.021279</td>\n",
              "      <td>0.557218</td>\n",
              "      <td>0.011098</td>\n",
              "      <td>0.040216</td>\n",
              "      <td>0.002240</td>\n",
              "      <td>0.011403</td>\n",
              "      <td>0.034413</td>\n",
              "      <td>0.005192</td>\n",
              "      <td>0.071065</td>\n",
              "      <td>0.079821</td>\n",
              "      <td>0.004582</td>\n",
              "      <td>0.007941</td>\n",
              "      <td>0.004276</td>\n",
              "      <td>0.574018</td>\n",
              "      <td>0.000916</td>\n",
              "      <td>0.005091</td>\n",
              "      <td>0.03146</td>\n",
              "      <td>0.008450</td>\n",
              "      <td>0.013846</td>\n",
              "      <td>0.059662</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>std</th>\n",
              "      <td>12.918058</td>\n",
              "      <td>0.412101</td>\n",
              "      <td>0.780701</td>\n",
              "      <td>0.804660</td>\n",
              "      <td>2.874148</td>\n",
              "      <td>1.015107</td>\n",
              "      <td>1.721212</td>\n",
              "      <td>1.011156</td>\n",
              "      <td>1.606287</td>\n",
              "      <td>1.896070</td>\n",
              "      <td>0.961955</td>\n",
              "      <td>1.710674</td>\n",
              "      <td>1.779238</td>\n",
              "      <td>1.609139</td>\n",
              "      <td>1.984152</td>\n",
              "      <td>1.645968</td>\n",
              "      <td>1.723377</td>\n",
              "      <td>2.279839</td>\n",
              "      <td>1.814406</td>\n",
              "      <td>0.786792</td>\n",
              "      <td>1.106349</td>\n",
              "      <td>1.846703</td>\n",
              "      <td>1.748025</td>\n",
              "      <td>1.684008</td>\n",
              "      <td>1.742410</td>\n",
              "      <td>1.321487</td>\n",
              "      <td>1.534163</td>\n",
              "      <td>1.944900</td>\n",
              "      <td>1.298229</td>\n",
              "      <td>3.093127</td>\n",
              "      <td>3.093541</td>\n",
              "      <td>1.543980</td>\n",
              "      <td>1.213627</td>\n",
              "      <td>1.596842</td>\n",
              "      <td>2.000374</td>\n",
              "      <td>2.002960</td>\n",
              "      <td>2.073125</td>\n",
              "      <td>1.871365</td>\n",
              "      <td>1.950625</td>\n",
              "      <td>1.173771</td>\n",
              "      <td>...</td>\n",
              "      <td>2.921736</td>\n",
              "      <td>0.566108</td>\n",
              "      <td>0.888518</td>\n",
              "      <td>0.237556</td>\n",
              "      <td>0.200885</td>\n",
              "      <td>0.604350</td>\n",
              "      <td>0.215408</td>\n",
              "      <td>0.810899</td>\n",
              "      <td>0.910574</td>\n",
              "      <td>0.188699</td>\n",
              "      <td>0.213712</td>\n",
              "      <td>0.375350</td>\n",
              "      <td>1.881271</td>\n",
              "      <td>0.057058</td>\n",
              "      <td>0.244210</td>\n",
              "      <td>0.157198</td>\n",
              "      <td>0.211487</td>\n",
              "      <td>0.396983</td>\n",
              "      <td>0.492001</td>\n",
              "      <td>0.126058</td>\n",
              "      <td>0.144319</td>\n",
              "      <td>0.608575</td>\n",
              "      <td>0.129928</td>\n",
              "      <td>0.223622</td>\n",
              "      <td>0.068402</td>\n",
              "      <td>0.116251</td>\n",
              "      <td>0.249706</td>\n",
              "      <td>0.109954</td>\n",
              "      <td>0.267432</td>\n",
              "      <td>0.384431</td>\n",
              "      <td>0.067535</td>\n",
              "      <td>0.088764</td>\n",
              "      <td>0.071224</td>\n",
              "      <td>0.561255</td>\n",
              "      <td>0.030258</td>\n",
              "      <td>0.077996</td>\n",
              "      <td>0.20907</td>\n",
              "      <td>0.092647</td>\n",
              "      <td>0.117728</td>\n",
              "      <td>0.236872</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>min</th>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25%</th>\n",
              "      <td>10.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>2.000000</td>\n",
              "      <td>2.000000</td>\n",
              "      <td>3.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>4.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>2.000000</td>\n",
              "      <td>5.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>2.000000</td>\n",
              "      <td>3.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>2.000000</td>\n",
              "      <td>3.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>2.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>2.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>2.000000</td>\n",
              "      <td>2.000000</td>\n",
              "      <td>5.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>5.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>2.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50%</th>\n",
              "      <td>30.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>3.000000</td>\n",
              "      <td>3.000000</td>\n",
              "      <td>7.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>5.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>3.000000</td>\n",
              "      <td>6.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>2.000000</td>\n",
              "      <td>2.000000</td>\n",
              "      <td>3.000000</td>\n",
              "      <td>4.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>3.000000</td>\n",
              "      <td>5.000000</td>\n",
              "      <td>2.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>3.000000</td>\n",
              "      <td>2.000000</td>\n",
              "      <td>2.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>2.000000</td>\n",
              "      <td>2.000000</td>\n",
              "      <td>4.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>4.000000</td>\n",
              "      <td>5.000000</td>\n",
              "      <td>6.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>2.000000</td>\n",
              "      <td>7.000000</td>\n",
              "      <td>2.000000</td>\n",
              "      <td>2.000000</td>\n",
              "      <td>4.000000</td>\n",
              "      <td>3.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>5.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>2.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75%</th>\n",
              "      <td>35.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>3.000000</td>\n",
              "      <td>3.000000</td>\n",
              "      <td>8.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>6.000000</td>\n",
              "      <td>2.000000</td>\n",
              "      <td>4.000000</td>\n",
              "      <td>7.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>3.000000</td>\n",
              "      <td>3.000000</td>\n",
              "      <td>4.000000</td>\n",
              "      <td>6.000000</td>\n",
              "      <td>2.000000</td>\n",
              "      <td>4.000000</td>\n",
              "      <td>6.000000</td>\n",
              "      <td>3.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>4.000000</td>\n",
              "      <td>3.000000</td>\n",
              "      <td>3.000000</td>\n",
              "      <td>2.000000</td>\n",
              "      <td>2.000000</td>\n",
              "      <td>3.000000</td>\n",
              "      <td>5.000000</td>\n",
              "      <td>2.000000</td>\n",
              "      <td>7.000000</td>\n",
              "      <td>7.000000</td>\n",
              "      <td>7.000000</td>\n",
              "      <td>2.000000</td>\n",
              "      <td>3.000000</td>\n",
              "      <td>8.000000</td>\n",
              "      <td>4.000000</td>\n",
              "      <td>4.000000</td>\n",
              "      <td>5.000000</td>\n",
              "      <td>4.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>6.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>4.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>max</th>\n",
              "      <td>41.000000</td>\n",
              "      <td>10.000000</td>\n",
              "      <td>6.000000</td>\n",
              "      <td>6.000000</td>\n",
              "      <td>10.000000</td>\n",
              "      <td>9.000000</td>\n",
              "      <td>9.000000</td>\n",
              "      <td>5.000000</td>\n",
              "      <td>9.000000</td>\n",
              "      <td>9.000000</td>\n",
              "      <td>7.000000</td>\n",
              "      <td>9.000000</td>\n",
              "      <td>9.000000</td>\n",
              "      <td>9.000000</td>\n",
              "      <td>9.000000</td>\n",
              "      <td>9.000000</td>\n",
              "      <td>9.000000</td>\n",
              "      <td>9.000000</td>\n",
              "      <td>9.000000</td>\n",
              "      <td>5.000000</td>\n",
              "      <td>9.000000</td>\n",
              "      <td>9.000000</td>\n",
              "      <td>9.000000</td>\n",
              "      <td>9.000000</td>\n",
              "      <td>9.000000</td>\n",
              "      <td>9.000000</td>\n",
              "      <td>9.000000</td>\n",
              "      <td>9.000000</td>\n",
              "      <td>9.000000</td>\n",
              "      <td>9.000000</td>\n",
              "      <td>9.000000</td>\n",
              "      <td>9.000000</td>\n",
              "      <td>9.000000</td>\n",
              "      <td>9.000000</td>\n",
              "      <td>9.000000</td>\n",
              "      <td>9.000000</td>\n",
              "      <td>9.000000</td>\n",
              "      <td>9.000000</td>\n",
              "      <td>9.000000</td>\n",
              "      <td>9.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>9.000000</td>\n",
              "      <td>7.000000</td>\n",
              "      <td>7.000000</td>\n",
              "      <td>9.000000</td>\n",
              "      <td>5.000000</td>\n",
              "      <td>7.000000</td>\n",
              "      <td>6.000000</td>\n",
              "      <td>6.000000</td>\n",
              "      <td>9.000000</td>\n",
              "      <td>6.000000</td>\n",
              "      <td>3.000000</td>\n",
              "      <td>7.000000</td>\n",
              "      <td>8.000000</td>\n",
              "      <td>3.000000</td>\n",
              "      <td>6.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>6.000000</td>\n",
              "      <td>5.000000</td>\n",
              "      <td>2.000000</td>\n",
              "      <td>5.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>12.000000</td>\n",
              "      <td>5.000000</td>\n",
              "      <td>8.000000</td>\n",
              "      <td>4.000000</td>\n",
              "      <td>3.000000</td>\n",
              "      <td>6.000000</td>\n",
              "      <td>6.000000</td>\n",
              "      <td>3.000000</td>\n",
              "      <td>8.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>2.000000</td>\n",
              "      <td>7.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>2.000000</td>\n",
              "      <td>4.00000</td>\n",
              "      <td>2.000000</td>\n",
              "      <td>2.000000</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>8 rows Ã— 86 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "           MOSTYPE     MAANTHUI  ...     ABYSTAND      CARAVAN\n",
              "count  9822.000000  9822.000000  ...  9822.000000  9822.000000\n",
              "mean     24.253207     1.108735  ...     0.013846     0.059662\n",
              "std      12.918058     0.412101  ...     0.117728     0.236872\n",
              "min       1.000000     1.000000  ...     0.000000     0.000000\n",
              "25%      10.000000     1.000000  ...     0.000000     0.000000\n",
              "50%      30.000000     1.000000  ...     0.000000     0.000000\n",
              "75%      35.000000     1.000000  ...     0.000000     0.000000\n",
              "max      41.000000    10.000000  ...     2.000000     1.000000\n",
              "\n",
              "[8 rows x 86 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Uo7AsnEBnV_7"
      },
      "source": [
        "Further more, we can see that the mean of CARAVAN column is very low as it is 0.059662 and the number of rows we have in total are 9822. This means that only about 6 percent of the CARVAN column data has 1s and the 94 percent of the CARVAN column data is 0s.As discussed before, this indicates that CARVAN has a lot of 0s in its columns in comparision to values of 1s. This further helps us understand that we are dealing with a very unbalanced dataset. Even more, based of the table displayed above, we can also see that the column MOSTYPE seems to have a much larger range of values for its domain than the other features. This feature may be more influential than the rest for this reason in some cases."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tI07j4G0ibEu"
      },
      "source": [
        "# **Models**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xOW5SEJeb83y"
      },
      "source": [
        "### Stepwise Feature Selection Based on P values\n",
        "\n",
        "I am doing Stepwise Feature Selection here and the data I get from thsi will be later used in the models section."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "24FlXIT8b7dM",
        "outputId": "00a05546-9fd0-4103-b7fa-16aba73c84a1"
      },
      "source": [
        "y = aa['CARAVAN']\n",
        "X = aa.drop(['CARAVAN'], axis=1)\n",
        "\n",
        "\n",
        "def stepwise_selection(X, y, \n",
        "                       initial_list=[], \n",
        "                       threshold_in=0.01, \n",
        "                       threshold_out = 0.05, \n",
        "                       verbose=True):\n",
        "    \"\"\" Perform a forward-backward feature selection \n",
        "    based on p-value from statsmodels.api.OLS\n",
        "    Arguments:\n",
        "        X - pandas.DataFrame with candidate features\n",
        "        y - list-like with the target\n",
        "        initial_list - list of features to start with (column names of X)\n",
        "        threshold_in - include a feature if its p-value < threshold_in\n",
        "        threshold_out - exclude a feature if its p-value > threshold_out\n",
        "        verbose - whether to print the sequence of inclusions and exclusions\n",
        "    Returns: list of selected features \n",
        "    Always set threshold_in < threshold_out to avoid infinite looping.\n",
        "    See https://en.wikipedia.org/wiki/Stepwise_regression for the details\n",
        "    \"\"\"\n",
        "    included = list(initial_list)\n",
        "    while True:\n",
        "        changed=False\n",
        "        # forward step\n",
        "        excluded = list(set(X.columns)-set(included))\n",
        "        new_pval = pd.Series(index=excluded)\n",
        "        for new_column in excluded:\n",
        "            model = sm.OLS(y, sm.add_constant(pd.DataFrame(X[included+[new_column]]))).fit()\n",
        "            new_pval[new_column] = model.pvalues[new_column]\n",
        "        best_pval = new_pval.min()\n",
        "        if best_pval < threshold_in:\n",
        "#            best_feature = new_pval.argmin()\n",
        "            best_feature = new_pval.idxmin()\n",
        "            included.append(best_feature)\n",
        "            changed=True\n",
        "            if verbose:\n",
        "                print('Add  {:30} with p-value {:.6}'.format(best_feature, best_pval))\n",
        "\n",
        "        # backward step\n",
        "        model = sm.OLS(y, sm.add_constant(pd.DataFrame(X[included]))).fit()\n",
        "        # use all coefs except intercept\n",
        "        pvalues = model.pvalues.iloc[1:]\n",
        "        worst_pval = pvalues.max() # null if pvalues is empty\n",
        "        if worst_pval > threshold_out:\n",
        "            changed=True\n",
        "            worst_feature = pvalues.argmax()\n",
        "            included.remove(worst_feature)\n",
        "            if verbose:\n",
        "                print('Drop {:30} with p-value {:.6}'.format(worst_feature, worst_pval))\n",
        "        if not changed:\n",
        "            break\n",
        "    return included\n",
        "\n",
        "result = stepwise_selection(X, y)\n",
        "\n",
        "print('resulting features:')\n",
        "print(result)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:28: DeprecationWarning: The default dtype for empty Series will be 'object' instead of 'float64' in a future version. Specify a dtype explicitly to silence this warning.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Add  PPERSAUT                       with p-value 2.14684e-42\n",
            "Add  MKOOPKLA                       with p-value 1.36739e-21\n",
            "Add  PWAPART                        with p-value 3.66711e-15\n",
            "Add  APLEZIER                       with p-value 8.20766e-15\n",
            "Add  MOPLHOOG                       with p-value 4.25236e-06\n",
            "Add  PBRAND                         with p-value 3.92829e-06\n",
            "Add  MBERBOER                       with p-value 8.31838e-06\n",
            "Add  MRELGE                         with p-value 1.41977e-05\n",
            "Add  PWALAND                        with p-value 0.000361295\n",
            "Add  ABRAND                         with p-value 0.000937601\n",
            "Add  AZEILPL                        with p-value 0.00153041\n",
            "Add  MINK123M                       with p-value 0.00152554\n",
            "Add  PBYSTAND                       with p-value 0.00243579\n",
            "Add  PGEZONG                        with p-value 0.00485648\n",
            "Add  AGEZONG                        with p-value 0.00450709\n",
            "Add  MHHUUR                         with p-value 0.00630075\n",
            "resulting features:\n",
            "['PPERSAUT', 'MKOOPKLA', 'PWAPART', 'APLEZIER', 'MOPLHOOG', 'PBRAND', 'MBERBOER', 'MRELGE', 'PWALAND', 'ABRAND', 'AZEILPL', 'MINK123M', 'PBYSTAND', 'PGEZONG', 'AGEZONG', 'MHHUUR']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W1QVElx1htQC"
      },
      "source": [
        "The results array which we printed above contains all the featured that are signinficant to use for the logistic regression later. In the following ocde cell below, we will be creatin training data based on these 16 features selected in the results array to use in our Logistic Regression in the models section."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mlpoDcz2d1c6",
        "outputId": "791d07bb-c21d-4cac-d167-0ab919c602ec"
      },
      "source": [
        "# Creating a new Dataset called X1 and X1_test which only have features which are selected from the stepwise feature slection process done above.\n",
        "X1=X_train \n",
        "X1_test=X_test\n",
        "y1=y\n",
        "count=0\n",
        "for i in X.columns:\n",
        "  if i not in result: # Removing Every Feature not in the result array\n",
        "    X1=X1.drop([i], axis=1)\n",
        "    X1_test=X1_test.drop([i], axis=1)\n",
        "\n",
        "print(X1.head())\n",
        "print(X1.shape)\n",
        "print(X1_test.shape)\n",
        "#X1 is the x dataset and Y is y dataset used later in the Logistic regression with stepwise feature selection.\n",
        "#X1_std = StandardScaler().fit_transform(X1) #Standardized X1 data\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "   MRELGE  MOPLHOOG  MBERBOER  MHHUUR  ...  AGEZONG  ABRAND  AZEILPL  APLEZIER\n",
            "0       7         1         1       1  ...        0       1        0         0\n",
            "1       6         0         0       2  ...        0       1        0         0\n",
            "2       3         0         0       7  ...        0       1        0         0\n",
            "3       5         3         0       5  ...        0       1        0         0\n",
            "4       7         5         4       4  ...        0       1        0         0\n",
            "\n",
            "[5 rows x 16 columns]\n",
            "(5822, 16)\n",
            "(4000, 16)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rjZkOUUpiCRM"
      },
      "source": [
        "Based of the Data above, we can see that only the features from the results array are in our new X1 dataset as we removed all the other features from the original dataset that were not selected."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1WqCZ5TbludV"
      },
      "source": [
        "### PCA"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t_4mEFv2perM"
      },
      "source": [
        "We are doing PCA here and we will used the transformed data later in the models section to train and test our Random Forest Classifier."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 534
        },
        "id": "_2uWym8nltyc",
        "outputId": "7601bbbb-f170-47c2-982e-2cd4954adf19"
      },
      "source": [
        "all_vars = df.drop(['ORIGIN'], axis=1)\n",
        "x_vars = all_vars.drop(['CARAVAN'], axis=1)\n",
        "y = df['CARAVAN']\n",
        "\n",
        "# principal component analysis \n",
        "\n",
        "# standardize the data \n",
        "X_std = StandardScaler().fit_transform(x_vars)\n",
        "mean_vec = np.mean(X_std, axis=0)\n",
        "cov_mat = (X_std - mean_vec).T.dot((X_std - mean_vec)) / (X_std.shape[0]-1)\n",
        "print('Covariance matrix \\n%s' %cov_mat)\n",
        "\n",
        "#Perform eigendecomposition on covariance matrix\n",
        "cov_mat = np.cov(X_std.T)\n",
        "eig_vals, eig_vecs = np.linalg.eig(cov_mat)\n",
        "\n",
        "for ev in eig_vecs:\n",
        "    np.testing.assert_array_almost_equal(1.0, np.linalg.norm(ev))\n",
        "print('Everything ok!')\n",
        "pca = PCA().fit(all_vars)\n",
        "plt.plot(np.cumsum(pca.explained_variance_ratio_))\n",
        "plt.xlabel('number of components')\n",
        "plt.ylabel('cumulative explained variance');\n",
        "plt.plot(np.cumsum(pca.explained_variance_ratio_))\n",
        "plt.xlabel('number of components')\n",
        "plt.ylabel('cumulative explained variance');"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Covariance matrix \n",
            "[[ 1.00010182 -0.04033167 -0.0065945  ... -0.0255354  -0.01633792\n",
            "  -0.04375385]\n",
            " [-0.04033167  1.00010182 -0.00431565 ... -0.01252764  0.0319394\n",
            "  -0.005852  ]\n",
            " [-0.0065945  -0.00431565  1.00010182 ...  0.01848839  0.01092878\n",
            "   0.03085877]\n",
            " ...\n",
            " [-0.0255354  -0.01252764  0.01848839 ...  1.00010182  0.00204416\n",
            "   0.00712214]\n",
            " [-0.01633792  0.0319394   0.01092878 ...  0.00204416  1.00010182\n",
            "   0.01727922]\n",
            " [-0.04375385 -0.005852    0.03085877 ...  0.00712214  0.01727922\n",
            "   1.00010182]]\n",
            "Everything ok!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8Pda1Yy1oK30",
        "outputId": "bda0822a-8c52-43f2-9f7c-382dae29db0b"
      },
      "source": [
        "pca = PCA(0.75).fit(X_train)\n",
        "pca.n_components_"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WqnhDojgpozJ"
      },
      "source": [
        "I am picking the .75 as a threshold for PCA.fit() because as we can see from the grah above, the number of components starts increasing much faster while the increase in the cumlative explained variance percentage decreases rapidly. I assumed that the tradeoff we get at .75 for only 4 newly created componenets are good to use later."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LDEQpGuspb31"
      },
      "source": [
        "X_train_scaled_pca = pca.transform(X_train)\n",
        "X_test_scaled_pca = pca.transform(X_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HMeckRfQt587",
        "outputId": "fa34bf40-8e8a-4823-f5e4-5ed2f86094e5"
      },
      "source": [
        "X_train_scaled_pca.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(5822, 4)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_bCd9HuHs4rJ"
      },
      "source": [
        "We are creating training and testing datasets above based upon PCA transformation to be later used for Random FOrest Classifier. in the models section."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E0t8XVR3X1kv"
      },
      "source": [
        "### Logistic Regression"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BPTv946l2P73"
      },
      "source": [
        "I used the following source to write code for logistic regression:[Source](https://towardsdatascience.com/building-a-logistic-regression-in-python-step-by-step-becd4d56c9c8)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mIxLCKjGX6Qc",
        "outputId": "e8fe0716-3afc-4ba6-9a60-f4ca5c68c0e7"
      },
      "source": [
        "from sklearn import preprocessing\n",
        "import matplotlib.pyplot as plt \n",
        "from sklearn.linear_model import LogisticRegression\n",
        "import sklearn\n",
        "# DOing logistic regression on orginal data.\n",
        "logreg = LogisticRegression()\n",
        "logreg.fit(X_train, y_train)\n",
        "y_pred = logreg.predict(X_test)\n",
        "print(\"Accuracy of Logistic regression\")\n",
        "print(sklearn.metrics.accuracy_score(y_test, y_pred))\n",
        "print(\"\\n Precision of Logistic regression\")\n",
        "print(sklearn.metrics.precision_score(y_test, y_pred))\n",
        "print(\"\\n Recall of Logistic regression\")\n",
        "print(sklearn.metrics.recall_score(y_test, y_pred))\n",
        "print(\"\\n F1 score of Logistic regression\")\n",
        "print(sklearn.metrics.f1_score(y_test, y_pred))\n",
        "print(\"\\n \\n\")\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy of Logistic regression\n",
            "0.94075\n",
            "\n",
            " Precision of Logistic regression\n",
            "0.6\n",
            "\n",
            " Recall of Logistic regression\n",
            "0.012605042016806723\n",
            "\n",
            " F1 score of Logistic regression\n",
            "0.02469135802469136\n",
            "\n",
            " \n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BEhs8S5O_rrC"
      },
      "source": [
        "The normal logistic regression has a acuracy of 0.94075 with a Precision of 0.6, recall of 0.01261, and a fscore of 0.02469. This logistic regression classifier was trained on data with 85 features."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Toatyh0f4hsL"
      },
      "source": [
        "### Logistic regression with stepwise selection based on p-values"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TTDl6SR_4uMx",
        "outputId": "88cb4eda-82ac-4488-e306-5ec9fac6c396"
      },
      "source": [
        "logreg1 = LogisticRegression()\n",
        "logreg1.fit(X1, y_train)\n",
        "y_pred1 = logreg1.predict(X1_test)\n",
        "print(\"Accuracy of Logistic regression with Stepwise selection\")\n",
        "print(sklearn.metrics.accuracy_score(y_test, y_pred1))\n",
        "print(\"\\n Precision of Logistic regression with Stepwise selection\")\n",
        "print(sklearn.metrics.precision_score(y_test, y_pred1))\n",
        "print(\"\\n Recall of Logistic regression with Stepwise selection\")\n",
        "print(sklearn.metrics.recall_score(y_test, y_pred1))\n",
        "print(\"\\n F1 score of Logistic regression with Stepwise selection\")\n",
        "print(sklearn.metrics.f1_score(y_test, y_pred1))\n",
        "print(\"\\n \\n\")\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy of Logistic regression with Stepwise selection\n",
            "0.9405\n",
            "\n",
            " Precision of Logistic regression with Stepwise selection\n",
            "0.5\n",
            "\n",
            " Recall of Logistic regression with Stepwise selection\n",
            "0.012605042016806723\n",
            "\n",
            " F1 score of Logistic regression with Stepwise selection\n",
            "0.024590163934426233\n",
            "\n",
            " \n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QQ04M7wZ_Jeh"
      },
      "source": [
        "The logistic regression with Stepwise selection based on p values has a accuracy of 0.9405 with a Precision of 0.5, recall of 0.01261, and a fscore of 0.02459. This classifier was trained on 16 features which were selected from the results array all the way back in Stepwise Feature Selection Based on P values.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6GDmNPIzrc9C"
      },
      "source": [
        "### Random Forest With PCA"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ql4XH_EgUbP8",
        "outputId": "bcc79e89-4b5f-4cb6-a77d-f45a4f60d037"
      },
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "Y_train=np.array(y_train)\n",
        "rfc = RandomForestClassifier()\n",
        "rfc.fit(X_train_scaled_pca, y_train)\n",
        "Y_predRFCPCA=rfc.predict(X_test_scaled_pca)\n",
        "\n",
        "\n",
        "print(\"Accuracy of our Random Forest Classifier(PCA) on test dataset\")\n",
        "print(sklearn.metrics.accuracy_score(y_test,  Y_predRFCPCA))\n",
        "print(\"\\n Precision of our Random Forest Classifier(PCA) on test dataset\")\n",
        "print(sklearn.metrics.precision_score(y_test,  Y_predRFCPCA))\n",
        "print(\"\\n Recall of our Random Forest Classifier(PCA) on test dataset\")\n",
        "print(sklearn.metrics.recall_score(y_test, Y_predRFCPCA))\n",
        "print(\"\\n F1 score of our Random Forest Classifier(PCA) on test dataset\")\n",
        "print(sklearn.metrics.f1_score(y_test,  Y_predRFCPCA))\n",
        "print(\"\\n \\n\")\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy of our Random Forest Classifier(PCA) on test dataset\n",
            "0.93225\n",
            "\n",
            " Precision of our Random Forest Classifier(PCA) on test dataset\n",
            "0.273972602739726\n",
            "\n",
            " Recall of our Random Forest Classifier(PCA) on test dataset\n",
            "0.08403361344537816\n",
            "\n",
            " F1 score of our Random Forest Classifier(PCA) on test dataset\n",
            "0.12861736334405144\n",
            "\n",
            " \n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tht_WDU0Nzak"
      },
      "source": [
        "The Random Forest CLassifier with PCA was trained and tested with data based upon only 4 PCA components which we got from doing PCA on our original data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IhktrfFM7S9s"
      },
      "source": [
        "### Random Forest based on feature importance feature selection"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d8UXytkaBRu1"
      },
      "source": [
        "Please Note that I used the following source to get the code for this section:[Source](https://chrisalbon.com/machine_learning/trees_and_forests/feature_selection_using_random_forest/)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l--0Sy0H7cJ0",
        "outputId": "e2592664-9bda-408d-c5f7-3953ef91e151"
      },
      "source": [
        "# Create a random forest classifier\n",
        "clf = RandomForestClassifier()\n",
        "\n",
        "# Train the classifier\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "# Print the name and gini importance of each feature\n",
        "feat_labels=X_train.columns\n",
        "for feature in zip(feat_labels, clf.feature_importances_):\n",
        "    print(feature)\n",
        "\n",
        "\n",
        "sfm = SelectFromModel(clf, threshold=0.04)\n",
        "\n",
        "# Train the selector\n",
        "sfm.fit(X_train, y_train)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "('MOSTYPE', 0.03927266647547531)\n",
            "('MAANTHUI', 0.0055257581074749105)\n",
            "('MGEMOMV', 0.009518393876906764)\n",
            "('MGEMLEEF', 0.011859208543828048)\n",
            "('MOSHOOFD', 0.02113995175491245)\n",
            "('MGODRK', 0.01043044806167484)\n",
            "('MGODPR', 0.018883361225282263)\n",
            "('MGODOV', 0.012859226557693901)\n",
            "('MGODGE', 0.018646911646539914)\n",
            "('MRELGE', 0.014164798890611221)\n",
            "('MRELSA', 0.009897751999971045)\n",
            "('MRELOV', 0.01230210085755439)\n",
            "('MFALLEEN', 0.014353964791900117)\n",
            "('MFGEKIND', 0.01825389574274616)\n",
            "('MFWEKIND', 0.017892726424798857)\n",
            "('MOPLHOOG', 0.018141475701919655)\n",
            "('MOPLMIDD', 0.01892604308007386)\n",
            "('MOPLLAAG', 0.01727086293701907)\n",
            "('MBERHOOG', 0.01585231609763914)\n",
            "('MBERZELF', 0.00899051348680064)\n",
            "('MBERBOER', 0.006558388772999742)\n",
            "('MBERMIDD', 0.018724127720844534)\n",
            "('MBERARBG', 0.017728160342541963)\n",
            "('MBERARBO', 0.015392983458748455)\n",
            "('MSKA', 0.014465874225388599)\n",
            "('MSKB1', 0.0153529651172433)\n",
            "('MSKB2', 0.015239092721105957)\n",
            "('MSKC', 0.017073751984391466)\n",
            "('MSKD', 0.011437892751548365)\n",
            "('MHHUUR', 0.01606779728270928)\n",
            "('MHKOOP', 0.015324084118065049)\n",
            "('MAUT1', 0.013576511227265173)\n",
            "('MAUT2', 0.011069877324474627)\n",
            "('MAUT0', 0.012163373544041008)\n",
            "('MZFONDS', 0.013659368286014491)\n",
            "('MZPART', 0.015222686802121263)\n",
            "('MINKM30', 0.014627291284037138)\n",
            "('MINK3045', 0.01928258863250568)\n",
            "('MINK4575', 0.01592984476692679)\n",
            "('MINK7512', 0.015058750938422216)\n",
            "('MINK123M', 0.005894620422222251)\n",
            "('MINKGEM', 0.014978948898988022)\n",
            "('MKOOPKLA', 0.02303549007783817)\n",
            "('PWAPART', 0.0265071929719252)\n",
            "('PWABEDR', 0.0031845540118037225)\n",
            "('PWALAND', 0.0007059080019441744)\n",
            "('PPERSAUT', 0.04188856313539249)\n",
            "('PBESAUT', 0.0012200844755666511)\n",
            "('PMOTSCO', 0.008062524424702216)\n",
            "('PVRAAUT', 1.1058684823778568e-05)\n",
            "('PAANHANG', 0.0023326254747626715)\n",
            "('PTRACTOR', 0.002466110329272455)\n",
            "('PWERKT', 5.7387486429707946e-05)\n",
            "('PBROM', 0.008545916759784961)\n",
            "('PLEVEN', 0.01147332763203888)\n",
            "('PPERSONG', 0.00038895100320236145)\n",
            "('PGEZONG', 0.003897324784804241)\n",
            "('PWAOREG', 0.0028961194547431515)\n",
            "('PBRAND', 0.05180145101518236)\n",
            "('PZEILPL', 0.0007144072910937516)\n",
            "('PPLEZIER', 0.011098427635347177)\n",
            "('PFIETS', 0.006463207253755241)\n",
            "('PINBOED', 0.002752839620876393)\n",
            "('PBYSTAND', 0.008396193993442333)\n",
            "('AWAPART', 0.0195440075927533)\n",
            "('AWABEDR', 0.002071030513350655)\n",
            "('AWALAND', 0.0005477949458803052)\n",
            "('APERSAUT', 0.0436363898801159)\n",
            "('ABESAUT', 0.001128554517705125)\n",
            "('AMOTSCO', 0.007749456239206408)\n",
            "('AVRAAUT', 0.0)\n",
            "('AAANHANG', 0.002189835030272484)\n",
            "('ATRACTOR', 0.001950703665313613)\n",
            "('AWERKT', 1.7896873447409922e-05)\n",
            "('ABROM', 0.00666562381401066)\n",
            "('ALEVEN', 0.013709069114319329)\n",
            "('APERSONG', 0.0003631339392011827)\n",
            "('AGEZONG', 0.002761041515198811)\n",
            "('AWAOREG', 0.0026006052837485093)\n",
            "('ABRAND', 0.019886118878055817)\n",
            "('AZEILPL', 0.00032105380716803626)\n",
            "('APLEZIER', 0.009375661418787309)\n",
            "('AFIETS', 0.009657278657279708)\n",
            "('AINBOED', 0.0022104662493726175)\n",
            "('ABYSTAND', 0.006703255662652952)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "SelectFromModel(estimator=RandomForestClassifier(bootstrap=True, ccp_alpha=0.0,\n",
              "                                                 class_weight=None,\n",
              "                                                 criterion='gini',\n",
              "                                                 max_depth=None,\n",
              "                                                 max_features='auto',\n",
              "                                                 max_leaf_nodes=None,\n",
              "                                                 max_samples=None,\n",
              "                                                 min_impurity_decrease=0.0,\n",
              "                                                 min_impurity_split=None,\n",
              "                                                 min_samples_leaf=1,\n",
              "                                                 min_samples_split=2,\n",
              "                                                 min_weight_fraction_leaf=0.0,\n",
              "                                                 n_estimators=100, n_jobs=None,\n",
              "                                                 oob_score=False,\n",
              "                                                 random_state=None, verbose=0,\n",
              "                                                 warm_start=False),\n",
              "                max_features=None, norm_order=1, prefit=False, threshold=0.04)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d9CnWgxIB4-D",
        "outputId": "02871b4c-86a7-43c0-f004-b08c5f603809"
      },
      "source": [
        "# Print the names of the most important features\n",
        "for feature_list_index in sfm.get_support(indices=True):\n",
        "    print(feat_labels[feature_list_index])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "PPERSAUT\n",
            "PBRAND\n",
            "APERSAUT\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AJR5Cx--z9l1"
      },
      "source": [
        "The following three features are selected above as features that are significant :PPERSAUT\n",
        "PBRAND,APERSAUT. This randomforest classifier will be trained and tested based upon these three features in the code cells below."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dKOQkfMfB-BW"
      },
      "source": [
        "X_important_train = sfm.transform(X_train) # Transforming our data based upon the results from feature selection.\n",
        "X_important_test = sfm.transform(X_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CW8lLYX1CEU6",
        "outputId": "f977fd9d-f3b5-4343-c024-2b5386449dab"
      },
      "source": [
        "clf_important = RandomForestClassifier()\n",
        "\n",
        "# Train the new classifier on the new dataset containing the most important features\n",
        "clf_important.fit(X_important_train, y_train) "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,\n",
              "                       criterion='gini', max_depth=None, max_features='auto',\n",
              "                       max_leaf_nodes=None, max_samples=None,\n",
              "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
              "                       min_samples_leaf=1, min_samples_split=2,\n",
              "                       min_weight_fraction_leaf=0.0, n_estimators=100,\n",
              "                       n_jobs=None, oob_score=False, random_state=None,\n",
              "                       verbose=0, warm_start=False)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PKTz8DV8GGrq",
        "outputId": "6b67bcee-e936-4207-fd82-e2ed28d0c6d4"
      },
      "source": [
        "\n",
        "y_important_pred = clf_important.predict(X_important_test)\n",
        "\n",
        "# View The Accuracy Of Our Limited Feature (2 Features) Model\n",
        "sklearn.metrics.accuracy_score(y_test, y_important_pred)\n",
        "print(\"Accuracy of our Random Forest Classifier based on feature importance feature selection on test dataset\")\n",
        "print(sklearn.metrics.accuracy_score(y_test,  y_important_pred))\n",
        "\n",
        "print(\"The precision, recall, and fscore with support are printed below\")\n",
        "sklearn.metrics.precision_recall_fscore_support(y_test, y_important_pred)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy of our Random Forest Classifier based on feature importance feature selection on test dataset\n",
            "0.9405\n",
            "The precision, recall, and fscore with support are printed below\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(array([0.9405, 0.    ]),\n",
              " array([1., 0.]),\n",
              " array([0.9693378, 0.       ]),\n",
              " array([3762,  238]))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 56
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8fiqqXMF98Q7"
      },
      "source": [
        "As we can see from above, the precision, fscore, and recall are all 0 for the Random forest classiier with feature selection. This tells us that we have 0 true postives in our predictions and that is a very bad sign as it doesn't help us identify what features of people help influence on why they have carvan insurance policy."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oIdvpuvqDnPb"
      },
      "source": [
        "# **Conclusion**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FOsQEbPpDrIS"
      },
      "source": [
        "## Comparision of the performance measures for all Models\n",
        "\n",
        "###  Logistic Regression\n",
        "The normal logistic regression has a acuracy of 0.94075 with a Precision of 0.6, recall of 0.01261, and a fscore of 0.02469. This logistic regression classifier was trained on data with 85 features. We have a relatively good accuracy of 0.94 but we only had a precision of 0.6. This tells us that only 60% of true predictions(predicting that CARVAN column value is 1) were actually true.\n",
        "However, we can see that recall is 0.0126 and this is significantly lower than the value we got for precision. This shows us that the number of false negatives in comparision true positives. This tells us that of those people who actually should be offered the Carvan insurance policy, our model was only able to correctly predict 1 % of those people who actually might be eligble to receive a carvan insurance policy.\n",
        "\n",
        "Now lets take a look at Logistic regression with Stepwise Feature Selection\n",
        "\n",
        "###  Logistic Regression With Stepwise Feature Selection based on p values\n",
        "This logistic regression classifier was trained on 16 features instead of all 85 features(Not including CARAVAN column). The logistic regression with Stepwise selection based on p values has a accuracy of 0.9405, Precision of 0.5, recall of 0.01261, and a fscore of 0.02459. While the the logistic regression classiefier has slightly lower precision, accuracy, fscore than the normal logistic regression we did, the logistic regression with Stepwise Feature Selection was trained on only 16 features in comparision to the normal logistic regrssion which was trained on 85 different features. Since we got very comparable results, that may mean that our Stepwis Feature Slection based upon p values may have gotten very important features which influence the data greatly.\n",
        "The 16 features we got from the Stepwise Feature Selection are 'PPERSAUT', 'MKOOPKLA', 'PWAPART', 'APLEZIER', 'MOPLHOOG', 'PBRAND', 'MBERBOER', 'MRELGE', 'PWALAND', 'ABRAND', 'AZEILPL', 'MINK123M', 'PBYSTAND', 'PGEZONG', 'AGEZONG', and 'MHHUUR'.\n",
        "\n",
        "Furthermore, now lets compare our results we got from our logistic regression classifiers with the results we got from the random forest classifiers which will be discussed below.\n",
        "\n",
        "###  RandomForest Classifier with PCA\n",
        "The PCA we did gave us 4 PCA componenets which we transformed our training and testing data with. Then, we trained and tested the RandomFOrest Classifier with this PCA transformed data. For our Random Forest Classifier(PCA) on test dataset, the accuracy was 0.93225, Precision was 0.273972602739726, Recall 0.08403361344537816, and F1 score was 0.12861736334405144.\n",
        "\n",
        "While we can see from our results that while our accuracy for this classifier was .93 adn as it was also similar to the accuracy of the logistic regression classifiers(0.94 on average), the precision for the Random Forest Classifier with PCA was only .27 and teh recall was 0.08. This tells us that the the number of true positives was very low in comparision to the number of true postives we got from the logistic regession classifiers because the logistic regression classifiers had a higher precision. However, the number of false negatives for this RandomForest Classifier with PCA might be slighly lower than the number of False negatives present in the results of the logistic regression classifiers because the recall for this RandomFOrest CLassifier was slightly higher(0.08>0.012). Thus, we can understand from this that the RandomForest classifier with PCA had a lower number of people falsely labeled as elegible to receive the Carvan insurance policy. This is good that the recall is higher because for an insurance company, it would cost them more to sign people who hav high risk factors than to potentially miss some people who could have been elgible customers. Overall, this RandomFOrest CLassifier with PCA did not perform well.\n",
        "\n",
        "###  RandomForest Classifier based on feature importance feature selection\n",
        "The RandomForest Classifier based on feature importance feature selection trained and tested on the following three important features selected from the Feature selection process which are PPERSAUT,PBRAND, and APERSAUT.\n",
        "\n",
        "The precision, recall, and the fscore are 0s for this. This could be because my threshold was set to .04. However, what this shows me is that this classifier had no true positives at all. The accuracy is preety high as it was 0.9405. However, this is misleading because the classifier only predicted well on the 0s in the CARAVN column. This classifier did not do well but that could be because of my input parameters in the feature selection process.\n",
        "\n",
        "\n",
        "###In Summary\n",
        "\n",
        "Overall, the logistic egression models seemd to do better than the random forest classifiers based on the comparision of the results done above. The logistic regression with stepwise feature selection did very similary in comparision to the normal logistic regression. The following 16 significantfeatures we got from the Stepwise Feature Selection are 'PPERSAUT', 'MKOOPKLA', 'PWAPART', 'APLEZIER', 'MOPLHOOG', 'PBRAND', 'MBERBOER', 'MRELGE', 'PWALAND', 'ABRAND', 'AZEILPL', 'MINK123M', 'PBYSTAND', 'PGEZONG', 'AGEZONG', and 'MHHUUR'. Overall, I would say, our models had very low recalls so there is a good chance that they could falsely predict a very high number of people as eligible for CARVAN insurance policy.\n"
      ]
    }
  ]
}